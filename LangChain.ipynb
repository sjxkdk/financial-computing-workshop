{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain: Coding AI Agents in Python\n",
    "\n",
    "#### By Pedro Izquierdo Lehmann\n",
    "\n",
    "Welcome to this hands-on introduction to **LangChain**! This notebook will guide you through building intelligent AI agents that can use tools, remember conversations, and make decisions autonomously.\n",
    "\n",
    "**What is LangChain?**\n",
    "LangChain is a framework for developing applications powered by language models. With it you can build explicit **chains**, which is an abstraction of an algorithm involving LLMs calls. Also, LangChain promotes implicit chains: instead of just asking an LLM questions, you can give it **tools** to use, and it will intelligently decide when and how to use them to answer your questions, instead of writing complex routing logic. \n",
    "\n",
    "LangChain works with the abstraction of the objects involved in the agentic system, such as\n",
    "\n",
    "- **Chains**: Abstraction of an algorithm involving multiple steps; a reusable workflow.\n",
    "- **Agents**: Abstraction of an LLM model equipped with tools, which can decide which tools/steps to run (an implicit chain).\n",
    "- **Tools**: Wrapped Python functions so the agent can call them.\n",
    "- **Memory/State**: Abstraction of context across conversation.\n",
    "\n",
    "LangChain orders these in **layers** of abstraction, so you can start simple and add power only when you need it. Each layer builds on the previous one. This notebook follows that same progression: we start with tools, then add memory, context, and structured outputs.\n",
    "\n",
    "**Content:**\n",
    "- Creating your first AI agent\n",
    "- Building custom tools for agents to use\n",
    "- Adding memory so agents remember past conversations\n",
    "- Using structured output for consistent responses\n",
    "- Context-aware tools that access user information\n",
    "- Best practices for production-ready agents\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## (0. Environment Setup)\n",
    "\n",
    "Before starting, you need to set up a Python virtual environment and install all required dependencies. Follow these steps:\n",
    "\n",
    "#### 1. Create a Virtual Environment\n",
    "\n",
    "Open your terminal and navigate to the **directory containing this notebook**, then run:\n",
    "\n",
    "```bash\n",
    "python3 -m venv lang-chain\n",
    "```\n",
    "\n",
    "This creates a virtual environment in a folder called `lang-chain`.\n",
    "\n",
    "#### 2. Activate the Virtual Environment\n",
    "\n",
    "**On macOS/Linux:**\n",
    "```bash\n",
    "source lang-chain/bin/activate\n",
    "```\n",
    "\n",
    "**On Windows:**\n",
    "```bash\n",
    "lang-chain\\Scripts\\activate\n",
    "```\n",
    "\n",
    "You should see `(lang-chain)` at the beginning of your terminal prompt, indicating the virtual environment is active.\n",
    "\n",
    "#### 3. Install Required Dependencies\n",
    "\n",
    "With the virtual environment activated, install all necessary packages:\n",
    "\n",
    "```bash\n",
    "pip install langchain langgraph langchain-anthropic langchain-openai jupyter ipykernel\n",
    "```\n",
    "\n",
    "This will install:\n",
    "- `langchain` - The core LangChain framework\n",
    "- `langgraph` - For building stateful agent workflows and checkpointers\n",
    "- `langchain-anthropic` - Anthropic (Claude) model provider\n",
    "- `langchain-openai` - OpenAI model provider\n",
    "- `jupyter` - Jupyter notebook environment\n",
    "- `ipykernel` - Jupyter kernel for the virtual environment\n",
    "\n",
    "Register the virtual environment as a Jupyter kernel:\n",
    "\n",
    "```bash\n",
    "python -m ipykernel install --user --name=lang-chain --display-name \"Python (lang-chain)\"\n",
    "```\n",
    "\n",
    "This ensures Jupyter can use your virtual environment's Python interpreter.\n",
    "\n",
    "#### 4. Start Jupyter Notebook\n",
    "\n",
    "We recommend two options to run the notebook:\n",
    "\n",
    "**Jupyter Notebook:**\n",
    "\n",
    "```bash\n",
    "jupyter notebook\n",
    "```\n",
    "\n",
    "This will open Jupyter in your web browser. Navigate to and open this notebook (`LangChain.ipynb`).\n",
    "\n",
    "**Code Editor like VS Code or Cursor:**\n",
    "\n",
    "1. Open the notebook file (`LangChain.ipynb`) in your code editor\n",
    "2. The editor should automatically detect it as a Jupyter notebook\n",
    "3. When prompted to select a kernel, choose **Python (lang-chain)** from the list\n",
    "4. If the kernel doesn't appear, you may need to refresh the kernel list or ensure the virtual environment is properly registered\n",
    "\n",
    "#### 5. Deactivate\n",
    "\n",
    "Don't forget to deactivate the virtual environment when you're done working with the following command:\n",
    "\n",
    "```bash\n",
    "deactivate\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python3 -m venv lang-chain\n",
    "# source lang-chain/bin/activate\n",
    "# lang-chain\\Scripts\\activate\n",
    "# pip install langchain langgraph langchain-anthropic langchain-openai jupyter ipykernel\n",
    "# python -m ipykernel install --user --name=lang-chain --display-name \"Python (lang-chain)\"\n",
    "# jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T21:58:52.096904Z",
     "start_time": "2026-01-16T21:58:46.121591Z"
    }
   },
   "source": [
    "# Import necessary libraries\n",
    "from langchain.agents import create_agent\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.tools import tool, ToolRuntime\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langchain.agents.structured_output import ToolStrategy\n",
    "from dataclasses import dataclass, field\n",
    "import os\n",
    "\n",
    "# Set API key\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "# Prompt user for the API key securely\n",
    "api_key = getpass.getpass(\"Enter your OpenAI API Key: \")\n",
    "\n",
    "# Set environment variable (recommended for LangChain)\n",
    "os.environ[\"OPENAI_API_KEY\"] = api_key\n"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Chains\n",
    "\n",
    "A **chain** is a sequence of steps (prompts, tools, or other chains) connected into a **single reusable pipeline**.\n",
    "\n",
    "- Think of it as a recipe: each step transforms the input and passes it to the next.\n",
    "- Chains can be simple (prompt -> LLM) or complex (multi-step reasoning + tools).\n",
    "- Agents *use* chains internally, but chains are **deterministic**: the steps are predefined.\n",
    "\n",
    "LangChain lets you build **chains explicitly** (deterministic pipelines) or **implicitly** through agents (dynamic pipelines).\n",
    "\n",
    "- **Explicit chain:** You wire together steps (prompt → model → parsing). The flow is fixed and repeatable.\n",
    "- **Agentic (implicit) chain:** The model decides which steps/tools to run at runtime. The flow can vary across calls. \n",
    "\n",
    "> **Note**: The cool thing about agent chains is that instead of just asking an LLM a question, you can give it **tools** to use, and it will decide when and how to use them to answer your question. For example, you don't need to write code that says \"if the user asks about weather, call the weather tool.\" The agent figures this out on its own!\n",
    "\n",
    "Below is an example of an explicit chain."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T21:58:58.765651Z",
     "start_time": "2026-01-16T21:58:55.069596Z"
    }
   },
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# Explicit chain: fixed four-step pipeline\n",
    "# Step 1: Prompt template\n",
    "chain_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\n",
    "        \"system\", # sets the global rule for output format: title line + exactly three bullets. It’s treated as higher‑priority instructions.\n",
    "        \"Return output with first line 'Title: {topic}', followed by exactly three bullet points.\"\n",
    "    ),\n",
    "    (\n",
    "        \"human\", # supplies the task input (the specific topic) and adds a content constraint (bullets must be full sentences).\n",
    "        \"Topic: {topic}. Each bullet must be a complete sentence.\"\n",
    "    )\n",
    "])\n",
    "\n",
    "# Step 2: Model call\n",
    "chain_model = init_chat_model(\"gpt-4.1-nano-2025-04-14\", temperature=0, api_key=api_key)\n",
    "\n",
    "# Step 3: Parse to string\n",
    "parser = StrOutputParser() # converts the raw LLM output into a string.\n",
    "\n",
    "# Step 4: Deterministic formatting\n",
    "format_output = RunnableLambda(\n",
    "    lambda s: \"\\n\".join(\n",
    "        [line for line in [\n",
    "            (\"Title: options pricing\" if not s.strip().split(\"\\n\")[0].startswith(\"Title:\") else s.strip().split(\"\\n\")[0]),\n",
    "            *[\n",
    "                (line if line.strip().startswith(\"-\") else f\"- {line.strip()}\")\n",
    "                for line in s.strip().split(\"\\n\")[1:]\n",
    "                if line.strip()\n",
    "            ][:3]\n",
    "        ] if line]\n",
    "    )\n",
    ")\n",
    "\n",
    "chain = chain_prompt | chain_model | parser | format_output\n",
    "result = chain.invoke({\"topic\": \"options pricing\"})\n",
    "print(result)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: options pricing\n",
      "- Options pricing involves determining the fair value of a financial derivative based on the underlying asset's price, volatility, time to expiration, and other factors.  \n",
      "- The Black-Scholes model is a widely used mathematical framework for calculating the theoretical price of European-style options.  \n",
      "- Market conditions and supply-demand dynamics can cause actual option prices to deviate from their theoretical values.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Build an Explicit Chain\n",
    "\n",
    "Create a chain that produces **three bullet points** about a finance topic. Use an explicit prompt + model pipeline, then invoke it.\n",
    "\n",
    "Hint: Use `ChatPromptTemplate`, compose with `|`, and access `response.content`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T21:59:41.688992Z",
     "start_time": "2026-01-16T21:59:37.448735Z"
    }
   },
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# EXERCISE: Build an explicit chain\n",
    "# 1. Create a ChatPromptTemplate with a {topic} variable\n",
    "# 2. Initialize a model with temperature=0\n",
    "# 3. Compose the chain with |\n",
    "# 4. Invoke it with topic=\"risk-neutral pricing\"\n",
    "# 5. Print response.content\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"Tell me three bullet points about {topic}.\")  # TODO: Fill this in\n",
    "model = init_chat_model(\"gpt-4o-mini\", temperature=0, api_key=api_key)  # TODO: Fill this in\n",
    "chain = prompt | model  # TODO: Fill this in\n",
    "response = chain.invoke({\"topic\": \"risk-neutral pricing\"})  # TODO: Fill this in\n",
    "\n",
    "print(response)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Sure! Here are three key points about risk-neutral pricing:\\n\\n1. **Concept of Risk Neutrality**: In risk-neutral pricing, investors are assumed to be indifferent to risk, meaning they require no additional return for taking on risk. This simplifies the valuation of financial derivatives by allowing the use of expected payoffs discounted at the risk-free rate.\\n\\n2. **Use in Derivative Pricing**: Risk-neutral pricing is a fundamental principle in the pricing of derivatives, such as options. The Black-Scholes model, for example, employs risk-neutral valuation to determine the fair price of options by calculating the expected payoff under a risk-neutral measure and discounting it back to present value.\\n\\n3. **Arbitrage-Free Pricing**: The risk-neutral pricing framework is built on the concept of no-arbitrage, which states that in an efficient market, there should be no opportunities to make a riskless profit. This principle ensures that the prices of financial instruments reflect their true value based on the underlying assets and their potential future payoffs.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 206, 'prompt_tokens': 17, 'total_tokens': 223, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_c4585b5b9c', 'id': 'chatcmpl-Cym8uXsJhYC8GuGmePqqvvLo5EEDG', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--019bc8d2-32ed-7cd3-8fa5-b2b6964f5f5d-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 17, 'output_tokens': 206, 'total_tokens': 223, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following will introduce chains implicitly as we build tool-driven workflows, then show how agents extend them with decision-making."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creating an Agent\n",
    "\n",
    "Let's start by creating a simple agent. You can think of an agent as a **chain with decision-making**: it interprets the user input, decides which tools to call (if any), and produces a final response.\n",
    "\n",
    "An agent needs:\n",
    "1. A **model** (the LLM that does the thinking)\n",
    "2. **Tools** (functions the agent can call)\n",
    "3. A **system prompt** (instructions for the agent)\n",
    "\n",
    "Here's a concrete example of how to create and use an agent with a time tool:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T21:59:47.021769Z",
     "start_time": "2026-01-16T21:59:45.271089Z"
    }
   },
   "source": [
    "# Example: Create an agent with a time tool\n",
    "def get_current_time(timezone: str = \"UTC\") -> str:\n",
    "    \"\"\"Get the current time in a specified timezone.\"\"\"\n",
    "    from datetime import datetime\n",
    "    return f\"Current time in {timezone}: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\"\n",
    "\n",
    "# Initialize the model\n",
    "example_model = init_chat_model(\"gpt-4.1-nano-2025-04-14\", temperature=0, api_key=api_key)\n",
    "\n",
    "# Create the agent\n",
    "example_agent = create_agent(\n",
    "    model=example_model,\n",
    "    tools=[get_current_time],\n",
    "    system_prompt=\"You are a helpful time assistant\"\n",
    ")\n",
    "\n",
    "# Invoke the agent\n",
    "example_response = example_agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"what time is it?\"}]}\n",
    ")\n",
    "\n",
    "# Access the response\n",
    "print(\"Example response:\", example_response['messages'][-1].content)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example response: The current time in UTC is 16:59 on January 16, 2026.\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Create a Basic Agent\n",
    "\n",
    "Now it's your turn! Create your first agent following the example above:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T22:00:01.917768Z",
     "start_time": "2026-01-16T22:00:00.551296Z"
    }
   },
   "source": [
    "# First, let's create a simple tool\n",
    "def get_weather(city: str) -> str:\n",
    "    \"\"\"Get weather for a given city.\"\"\"\n",
    "    return f\"It's always sunny in {city}!\"\n",
    "\n",
    "# EXERCISE: Create a basic agent with a weather tool\n",
    "# 1. Initialize a chat model\n",
    "# Hint: Use init_chat_model() from langchain.chat_models with model name \"gpt-4.1-nano-2025-04-14\" or \"gpt-4\"\n",
    "model = init_chat_model(\"gpt-4o-mini\", temperature=0, api_key=api_key)  # TODO: Fill this in\n",
    "\n",
    "# 2. Create an agent using create_agent\n",
    "# Hint: Use create_agent() from langchain.agents with model, tools=[get_weather], and system_prompt=\"You are a helpful assistant\"\n",
    "agent = create_agent(model, tools=[get_weather], system_prompt=\"You are a helpful assistant\")  # TODO: Fill this in\n",
    "\n",
    "# 3. Run the agent with a message asking about the weather in San Francisco\n",
    "# Hint: Use agent.invoke() with {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in San Francisco\"}]}\n",
    "response = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in San Francisco\"}]})  # TODO: Fill this in\n",
    "\n",
    "# Print the response\n",
    "print(response)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='what is the weather in San Francisco', additional_kwargs={}, response_metadata={}, id='0a6b607d-9f0e-4d55-85f3-3dad03b42168'), AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 57, 'total_tokens': 72, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_29330a9688', 'id': 'chatcmpl-Cym9IEJ3wzE7mNfUR7sYChtelzPXA', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--019bc8d2-8d37-7573-8bb8-1f681a025b6f-0', tool_calls=[{'name': 'get_weather', 'args': {'city': 'San Francisco'}, 'id': 'call_SZMgw436qLhi8rWnEuZkzfXY', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 57, 'output_tokens': 15, 'total_tokens': 72, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content=\"It's always sunny in San Francisco!\", name='get_weather', id='3a608c98-c7a6-4b8e-97f8-a062791abab4', tool_call_id='call_SZMgw436qLhi8rWnEuZkzfXY'), AIMessage(content='The weather in San Francisco is always sunny!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 87, 'total_tokens': 97, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_29330a9688', 'id': 'chatcmpl-Cym9IewgouStVoGiID3t6kbRQUz6f', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019bc8d2-9042-7b23-bddb-9d3efa3b5e36-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 87, 'output_tokens': 10, 'total_tokens': 97, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creating Custom Tools\n",
    "\n",
    "Tools are functions that agents can call. LangChain makes it easy to convert Python functions into tools using the `@tool` decorator. The `@tool` decorator:\n",
    "- Automatically extracts function name, description, and parameters\n",
    "- Makes the function available to the agent\n",
    "- Handles type validation and conversion\n",
    "\n",
    "> **Important**: The function's docstring becomes part of the agent's prompt! Make it descriptive so the agent knows when to use the tool.\n",
    "\n",
    "Here's a working example using different tools (string manipulation) to demonstrate the @tool decorator:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T22:00:33.349427Z",
     "start_time": "2026-01-16T22:00:11.560840Z"
    }
   },
   "source": [
    "# Example: Create tools for string manipulation (different from the calculator exercise)\n",
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def reverse_string(text: str) -> str:\n",
    "    \"\"\"Reverse a string.\"\"\"\n",
    "    return text[::-1]\n",
    "\n",
    "@tool\n",
    "def uppercase_string(text: str) -> str:\n",
    "    \"\"\"Convert a string to uppercase.\"\"\"\n",
    "    return text.upper()\n",
    "\n",
    "@tool\n",
    "def count_words(text: str) -> int:\n",
    "    \"\"\"Count the number of words in a string.\"\"\"\n",
    "    return len(text.split())\n",
    "\n",
    "# Create an agent with these string manipulation tools\n",
    "example_model = init_chat_model(\"gpt-4.1-nano-2025-04-14\", temperature=0, api_key=api_key)\n",
    "string_agent = create_agent(\n",
    "    model=example_model,\n",
    "    tools=[reverse_string, uppercase_string, count_words],\n",
    "    system_prompt=\"You are a helpful text processing assistant\"\n",
    ")\n",
    "\n",
    "# Test the agent\n",
    "example_response = string_agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Reverse the string 'Hello World' and count its words\"}]}\n",
    ")\n",
    "print(\"Example response:\", example_response['messages'][-1].content)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example response: The reversed string is \"dlroW olleH\" and it contains 2 words.\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Create a LangChain tools\n",
    "\n",
    "Now it's your turn! Create your first LangChain tools following the syntaxis of the example above:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T22:01:25.085037Z",
     "start_time": "2026-01-16T22:01:22.302646Z"
    }
   },
   "source": [
    "### Exercise 2: Create Multiple Tools\n",
    "\n",
    "# EXERCISE: Create a calculator agent with multiple tools\n",
    "# 1. Create a tool for addition\n",
    "# Hint: Use @tool decorator from langchain.tools, function should take (a: float, b: float) and return a + b\n",
    "@tool\n",
    "def add(a: float, b: float) -> float:\n",
    "    \"\"\"Add two numbers.\"\"\"\n",
    "    return a + b\n",
    "\n",
    "# 2. Create a tool for multiplication\n",
    "# Hint: Use @tool decorator, function should take (a: float, b: float) and return a * b\n",
    "@tool\n",
    "def multiply(a: float, b: float) -> float:\n",
    "    \"\"\"Multiply two numbers.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "# 3. Create a tool for getting the square root\n",
    "# Hint: Use @tool decorator and import math, function should take (x: float) and return math.sqrt(x)\n",
    "import math\n",
    "@tool\n",
    "def sqrt(x: float) -> float:\n",
    "    \"\"\"Calculate the square root of a number.\"\"\"\n",
    "    return math.sqrt(x)\n",
    "\n",
    "# 4. Create an agent with all three tools\n",
    "# Hint: Use create_agent() from langchain.agents with model, tools=[add, multiply, sqrt], and system_prompt=\"You are a helpful calculator assistant\"\n",
    "calculator_agent = create_agent(\n",
    "    model=model, \n",
    "    tools=[add, multiply, sqrt], \n",
    "    system_prompt=\"You are a helpful calculator assistant\"\n",
    ")\n",
    "\n",
    "# 5. Test your agent\n",
    "# Hint: Use calculator_agent.invoke() with {\"messages\": [{\"role\": \"user\", \"content\": \"What is 15 plus 27, then multiply that by 3?\"}]}\n",
    "response = calculator_agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"What is 15 plus 27, then multiply that by 3?\"}]})\n",
    "print(response)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='What is 15 plus 27, then multiply that by 3?', additional_kwargs={}, response_metadata={}, id='b18dd942-69ca-4054-a89a-4635e901b307'), AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 108, 'total_tokens': 125, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_3683ee3deb', 'id': 'chatcmpl-CymAbayjxE0tEVOXGevoThZlxRSfV', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--019bc8d3-cc93-7f43-83ec-88f3d157212d-0', tool_calls=[{'name': 'add', 'args': {'a': 15, 'b': 27}, 'id': 'call_77crbX2OJY0t6hoS29pR4mol', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 108, 'output_tokens': 17, 'total_tokens': 125, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content='42.0', name='add', id='59e5e654-d1cc-4c4b-b2c4-702370ebed1c', tool_call_id='call_77crbX2OJY0t6hoS29pR4mol'), AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 135, 'total_tokens': 152, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_3683ee3deb', 'id': 'chatcmpl-CymAczFNqGfNzPHw1Uep4VIDsS537', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--019bc8d3-d0d6-7d03-914f-3cde5e8488a1-0', tool_calls=[{'name': 'multiply', 'args': {'a': 42, 'b': 3}, 'id': 'call_LJ75uDkpJPTjbWgeCBMOqjee', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 135, 'output_tokens': 17, 'total_tokens': 152, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content='126.0', name='multiply', id='516c5968-06fc-4984-99af-768b0f0da3b8', tool_call_id='call_LJ75uDkpJPTjbWgeCBMOqjee'), AIMessage(content='15 plus 27 is 42, and multiplying that by 3 gives you 126.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 20, 'prompt_tokens': 162, 'total_tokens': 182, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_29330a9688', 'id': 'chatcmpl-CymAdq7LyRppGbOmO9Z1m8CCeBTVi', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019bc8d3-d3a2-74d3-b61b-7e9cb6722fca-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 162, 'output_tokens': 20, 'total_tokens': 182, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Behavior**: The agent should:\n",
    "1. First call `add(15, 27)` to get 42\n",
    "2. Then call `multiply(42, 3)` to get 126\n",
    "3. Return the final answer\n",
    "\n",
    "This demonstrates that agents can **chain multiple tool calls** to solve complex problems! The agent automatically figures out the sequence of operations needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Tools with Runtime Context\n",
    "\n",
    "Sometimes tools need access to runtime information (like user IDs, session data, etc.). LangChain provides `ToolRuntime` for this. `ToolRuntime` allows tools to access:\n",
    "- **Context**: Custom data passed when invoking the agent\n",
    "- **Memory**: Conversation history and state\n",
    "- **Configuration**: Runtime settings\n",
    "\n",
    "> **Note**: The `ToolRuntime` parameter is automatically injected by LangChain. You don't pass it when calling the tool - LangChain handles that for you!\n",
    "\n",
    "Here is an example that uses `ToolRuntime` to build Context-Aware Tools"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T22:02:47.441779Z",
     "start_time": "2026-01-16T22:02:46.050676Z"
    }
   },
   "source": [
    "# # Example: Context-aware tool for user preferences (different from greeting exercise)\n",
    "from dataclasses import dataclass\n",
    "from langchain.tools import tool, ToolRuntime\n",
    "\n",
    "# Define a context schema with user preferences\n",
    "@dataclass\n",
    "class UserContext:\n",
    "    \"\"\"Custom runtime context schema.\"\"\"\n",
    "    user_id: str\n",
    "    favorite_color: str\n",
    "\n",
    "# Create a tool that uses ToolRuntime to access user preferences\n",
    "@tool\n",
    "def get_recommendation(runtime: ToolRuntime[UserContext]) -> str:\n",
    "    \"\"\"Get a personalized recommendation based on user preferences.\"\"\"\n",
    "    user_id = runtime.context.user_id\n",
    "    favorite_color = runtime.context.favorite_color\n",
    "    return f\"User {user_id} might like items in {favorite_color} color!\"\n",
    "\n",
    "# Create an agent with this tool\n",
    "example_model = init_chat_model(\"gpt-4.1-nano-2025-04-14\", temperature=0, api_key=api_key)\n",
    "recommendation_agent = create_agent(\n",
    "    model=example_model,\n",
    "    tools=[get_recommendation],\n",
    "    system_prompt=\"You are a helpful recommendation assistant\",\n",
    "    context_schema=UserContext\n",
    ")\n",
    "\n",
    "# Invoke with context\n",
    "example_response = recommendation_agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"What would you recommend for me?\"}]},\n",
    "    context=UserContext(user_id=\"123\", favorite_color=\"blue\")\n",
    ")\n",
    "print(\"Example response:\", example_response['messages'][-1].content)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example response: Based on your preferences, I recommend exploring items in blue color. Would you like some specific suggestions or categories to consider?\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Context-Aware Tools\n",
    "\n",
    "Now create your own personalized greeting tool:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T22:04:08.045209Z",
     "start_time": "2026-01-16T22:04:06.415701Z"
    }
   },
   "source": [
    "# EXERCISE: Create a personalized greeting tool using runtime context\n",
    "# 1. Define a Context dataclass with a user_name field\n",
    "# Hint: Use @dataclass from dataclasses, create a class Context with user_name: str field\n",
    "@dataclass\n",
    "class Context:\n",
    "    user_name: str\n",
    "\n",
    "# 2. Create a tool that uses ToolRuntime to access context\n",
    "# Hint: Use @tool from langchain.tools, function parameter should be runtime: ToolRuntime[Context], access user_name via runtime.context.user_name\n",
    "@tool\n",
    "def get_personalized_greeting(runtime: ToolRuntime[Context]) -> str:\n",
    "    \"\"\"Get a personalized greeting based on the user's name.\"\"\"\n",
    "    return f\"Hello, {runtime.context.user_name}!\"\n",
    "\n",
    "# 3. Create an agent with this tool\n",
    "# Hint: Use create_agent() with model, tools=[get_personalized_greeting], system_prompt, and context_schema=Context\n",
    "personalized_agent = create_agent(\n",
    "    model=model,\n",
    "    tools=[get_personalized_greeting],\n",
    "    system_prompt=\"You are a helpful assistant who greets people by name.\",\n",
    "    context_schema=Context\n",
    ")\n",
    "\n",
    "# 4. Invoke the agent with context\n",
    "# Hint: Use personalized_agent.invoke() with messages and context=Context(user_name=\"Alice\")\n",
    "response = personalized_agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Please greet me.\"}]},\n",
    "    context=Context(user_name=\"Alice\")\n",
    ")\n",
    "print(response)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='Please greet me.', additional_kwargs={}, response_metadata={}, id='dd1d4773-8920-4b44-8ae4-a34f5c02f2d5'), AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 61, 'total_tokens': 75, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_29330a9688', 'id': 'chatcmpl-CymDGv0AlogKEEcNhUnNX9ab1G4Mz', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--019bc8d6-4da3-7563-8d16-394a9f5dff05-0', tool_calls=[{'name': 'get_personalized_greeting', 'args': {}, 'id': 'call_1kPQ2yfkeHlKxEWCcvNxXF9o', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 61, 'output_tokens': 14, 'total_tokens': 75, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content='Hello, Alice!', name='get_personalized_greeting', id='d65b249b-4223-4385-bff5-4a7a42e82b86', tool_call_id='call_1kPQ2yfkeHlKxEWCcvNxXF9o'), AIMessage(content='Hello, Alice! How are you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 91, 'total_tokens': 101, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_29330a9688', 'id': 'chatcmpl-CymDGaRRh2DnzrHONUTNxvJQXoqEj', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019bc8d6-517e-7691-9d9b-45f399ef1ec0-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 91, 'output_tokens': 10, 'total_tokens': 101, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Adding Memory to Agents\n",
    "\n",
    "So far, our agents don't remember previous conversations. Let's add **memory** so agents can maintain context across multiple interactions. A **checkpointer** stores conversation state, which you can think of as the **state of the agent's chain across turns**. LangChain provides:\n",
    "- `InMemorySaver`: For development/testing (lost when program ends)\n",
    "- Database checkpointers: For production (persistent storage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**: `ToolRuntime` and `InMemorySaver` both relate to “runtime context,” but they operate at different layers.\n",
    ">- `ToolRuntime` is per-tool-call and injected into tool functions; it provides a view of context/memory/config at that moment.\n",
    ">- `InMemorySaver` is storage, passed to the agent as a checkpointer to persist conversation state between invocations (keyed by `thread_id`). It does not get injected into tools.\n",
    ">- `ToolRuntime` doesn’t store anything by itself; `InMemorySaver` doesn’t provide arbitrary runtime context/config—only persistence for state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of how to add memory to an agent:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T22:04:16.692864Z",
     "start_time": "2026-01-16T22:04:15.021507Z"
    }
   },
   "source": [
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langchain.tools import tool\n",
    "\n",
    "# Create a checkpointer\n",
    "example_checkpointer = InMemorySaver()\n",
    "\n",
    "# Create a quote tool (different from the fact tool in the exercise)\n",
    "@tool\n",
    "def get_quote(category: str) -> str:\n",
    "    \"\"\"Get an inspirational quote by category.\"\"\"\n",
    "    quotes = {\n",
    "        \"success\": \"Success is not final, failure is not fatal: it is the courage to continue that counts.\",\n",
    "        \"wisdom\": \"The only true wisdom is in knowing you know nothing.\",\n",
    "        \"motivation\": \"The way to get started is to quit talking and begin doing.\"\n",
    "    }\n",
    "    return quotes.get(category.lower(), \"Here's a quote: Keep moving forward!\")\n",
    "\n",
    "# Create an agent with memory\n",
    "example_model = init_chat_model(\"gpt-4.1-nano-2025-04-14\", temperature=0, api_key=api_key)\n",
    "quote_agent = create_agent(\n",
    "    model=example_model,\n",
    "    tools=[get_quote],\n",
    "    system_prompt=\"You are a helpful assistant that shares inspirational quotes\",\n",
    "    checkpointer=example_checkpointer\n",
    ")\n",
    "\n",
    "# Create a config with thread_id\n",
    "example_config = {\"configurable\": {\"thread_id\": \"quote-session-1\"}}\n",
    "\n",
    "# First message\n",
    "example_response1 = quote_agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Give me a quote about success\"}]},\n",
    "    config=example_config\n",
    ")\n",
    "\n",
    "# Second message - agent remembers!\n",
    "example_response2 = quote_agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"What quote did you just share?\"}]},\n",
    "    config=example_config\n",
    ")\n",
    "print(\"First response:\", example_response1['messages'][-1].content)\n",
    "print(\"Second response:\", example_response2['messages'][-1].content)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First response: Here's an inspiring quote about success: \"Success is not final, failure is not fatal: it is the courage to continue that counts.\"\n",
      "Second response: I shared the quote: \"Success is not final, failure is not fatal: it is the courage to continue that counts.\"\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**: The `thread_id` in the config is crucial! It tells the checkpointer which conversation to load. Different `thread_id` values mean different conversations. This allows you to manage multiple concurrent conversations with the same agent.\n",
    "\n",
    "### Exercise 4: Conversational Memory\n",
    "\n",
    "Now create your own agent with memory:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T22:06:20.461705Z",
     "start_time": "2026-01-16T22:06:17.831530Z"
    }
   },
   "source": [
    "# EXERCISE: Create an agent with conversational memory\n",
    "# 1. Create an InMemorySaver checkpointer\n",
    "# Hint: Import InMemorySaver from langgraph.checkpoint.memory and create an instance\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "checkpointer = InMemorySaver()  # TODO: Fill this in\n",
    "\n",
    "# 2. Create a simple tool that returns a fact\n",
    "@tool\n",
    "def get_fact(topic: str) -> str:\n",
    "    \"\"\"Get an interesting fact about a topic.\"\"\"\n",
    "    facts = {\n",
    "        \"python\": \"Python was named after Monty Python's Flying Circus\",\n",
    "        \"ai\": \"The term 'artificial intelligence' was coined in 1956\",\n",
    "        \"space\": \"A day on Venus is longer than its year\"\n",
    "    }\n",
    "    return facts.get(topic.lower(), f\"I don't know much about {topic}\")\n",
    "\n",
    "# 3. Create an agent with the checkpointer\n",
    "# Hint: Use create_agent() with model, tools=[get_fact], system_prompt, and checkpointer=checkpointer\n",
    "memory_agent = create_agent(\n",
    "    model=model,\n",
    "    tools=[get_fact],\n",
    "    system_prompt=\"You are a helpful assistant.\",\n",
    "    checkpointer=checkpointer\n",
    ")  # TODO: Fill this in\n",
    "\n",
    "# 4. Create a config with a thread_id (this identifies the conversation)\n",
    "# Hint: Create a dictionary with {\"configurable\": {\"thread_id\": \"conversation-1\"}}\n",
    "config = {\"configurable\": {\"thread_id\": \"conversation-1\"}}  # TODO: Fill this in\n",
    "\n",
    "# 5. Ask the agent: \"Tell me a fact about Python\"\n",
    "# Hint: Use memory_agent.invoke() with messages and config\n",
    "response1 = memory_agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Tell me a fact about Python\"}]},\n",
    "    config=config\n",
    ")  # TODO: Fill this in\n",
    "print(\"First response:\", response1['messages'][-1].content)\n",
    "\n",
    "# 6. In a follow-up message, ask: \"What was the fact you just told me?\"\n",
    "# Hint: Use memory_agent.invoke() again with the same config - the agent should remember!\n",
    "response2 = memory_agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"What was the fact you just told me?\"}]},\n",
    "    config=config\n",
    ")  # TODO: Fill this in\n",
    "print(\"Second response:\", response2['messages'][-1].content)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First response: A fun fact about Python is that it was named after \"Monty Python's Flying Circus,\" the British comedy television show.\n",
      "Second response: The fact I shared is that Python was named after \"Monty Python's Flying Circus,\" the British comedy television show.\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Structured Output\n",
    "\n",
    "Sometimes you want the agent's response in a specific format. LangChain supports **structured output** using dataclasses or `Pydantic` models. Its functional object for this is `ToolStrategy`, which tells the agent to use tools AND return structured output. The agent will still use tools, but format its final response according to your schema. This gives you the best of both worlds - tool usage with predictable output formats.\n",
    "\n",
    "Here's how to create an agent with structured output:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T22:06:37.418098Z",
     "start_time": "2026-01-16T22:06:36.087544Z"
    }
   },
   "source": [
    "from dataclasses import dataclass, field\n",
    "from langchain.agents.structured_output import ToolStrategy\n",
    "from langchain.tools import tool\n",
    "\n",
    "# Define a different response format\n",
    "@dataclass\n",
    "class ProductRecommendation:\n",
    "    \"\"\"Response schema for product recommendations.\"\"\"\n",
    "    product_name: str\n",
    "    price: float\n",
    "    rating: float = 0.0\n",
    "    features: list[str] = field(default_factory=list)\n",
    "\n",
    "# Create a product search tool\n",
    "@tool\n",
    "def search_products(category: str) -> str:\n",
    "    \"\"\"Search for products in a category.\"\"\"\n",
    "    return f\"Found products in {category}: Laptop ($999, 4.5 stars), Tablet ($499, 4.2 stars)\"\n",
    "\n",
    "# Create an agent with structured output\n",
    "example_model = init_chat_model(\"gpt-4.1-nano-2025-04-14\", temperature=0, api_key=api_key)\n",
    "product_agent = create_agent(\n",
    "    model=example_model,\n",
    "    tools=[search_products],\n",
    "    system_prompt=\"You are a helpful product recommendation assistant\",\n",
    "    response_format=ToolStrategy(ProductRecommendation)\n",
    ")\n",
    "\n",
    "# Ask a question and get a structured response\n",
    "example_response = product_agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Recommend a laptop for me\"}]}\n",
    ")\n",
    "\n",
    "# Access the structured response\n",
    "structured = example_response['structured_response']\n",
    "print(\"Product Name:\", structured.product_name)\n",
    "print(\"Price:\", structured.price)\n",
    "print(\"Rating:\", structured.rating)\n",
    "print(\"Features:\", structured.features)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product Name: Laptop\n",
      "Price: 999.0\n",
      "Rating: 4.5\n",
      "Features: ['High performance', 'Lightweight design', 'Long battery life']\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Structured Responses\n",
    "\n",
    "Now create your own agent with structured output:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T22:07:40.072273Z",
     "start_time": "2026-01-16T22:07:37.320966Z"
    }
   },
   "source": [
    "# EXERCISE: Create an agent with structured output\n",
    "# 1. Define a ResponseFormat dataclass\n",
    "# Hint: Use @dataclass from dataclasses, include answer: str, confidence: float = 0.0, and sources: list[str] = field(default_factory=list)\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "@dataclass\n",
    "class ResponseFormat:\n",
    "    answer: str\n",
    "    confidence: float = 0.0\n",
    "    sources: list[str] = field(default_factory=list)\n",
    "\n",
    "# 2. Create a simple tool\n",
    "# Hint: Use @tool from langchain.tools, function should take (query: str) and return a string\n",
    "@tool\n",
    "def search_knowledge_base(query: str) -> str:\n",
    "    \"\"\"Search the internal knowledge base for information.\"\"\"\n",
    "    return \"LangChain is a framework for developing applications powered by language models.\"\n",
    "\n",
    "# 3. Create an agent with structured output\n",
    "# Hint: Use create_agent() with model, tools, system_prompt, and response_format=ToolStrategy(ResponseFormat) from langchain.agents.structured_output\n",
    "from langchain.agents.structured_output import ToolStrategy\n",
    "\n",
    "structured_agent = create_agent(\n",
    "    model=model,\n",
    "    tools=[search_knowledge_base],\n",
    "    system_prompt=\"You are a helpful research assistant.\",\n",
    "    response_format=ToolStrategy(ResponseFormat)\n",
    ")\n",
    "\n",
    "# 4. Ask a question and get a structured response\n",
    "# Hint: Use structured_agent.invoke() with messages\n",
    "response = structured_agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"What is LangChain?\"}]})\n",
    "\n",
    "# Access the structured response\n",
    "structured = response['structured_response']\n",
    "print(f\"Answer: {structured.answer}\")\n",
    "print(f\"Confidence: {structured.confidence}\")\n",
    "print(f\"Sources: {structured.sources}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: LangChain is a framework for developing applications powered by language models. It provides tools and components to facilitate the integration of language models into various applications, enabling developers to create more sophisticated and interactive experiences using natural language processing.\n",
      "Confidence: 0.9\n",
      "Sources: []\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Guided Exercise: Daily S&P 500 Decision with Twitter Sentiment Analysis\n",
    "\n",
    "In this exercise, you will build an agent that decides whether to **BUY** or **NOT BUY** units of the S&P 500 (e.g., SPY), computing **local sentiment** for tweets **before the decision day**. We use the following Hugging Face dataset: https://huggingface.co/datasets/StephanAkkerman/stock-market-tweets-data\n",
    "\n",
    "> **Note**: This is a simplified educational example; don’t take it as financial advice.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "# Build a local pandas dataframe (sampled)\n",
    "ds = load_dataset(\"StephanAkkerman/stock-market-tweets-data\", split=\"train\")\n",
    "df = ds.select(range(20000)).to_pandas()\n",
    "\n",
    "# Normalize and parse dates\n",
    "df.columns = [c.strip() for c in df.columns]\n",
    "df[\"created_at\"] = pd.to_datetime(df[\"created_at\"], errors=\"coerce\", utc=True)\n",
    "df = df.dropna(subset=[\"created_at\", \"text\"])\n",
    "df[\"created_at_date\"] = df[\"created_at\"].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local sentiment scoring\n",
    "positive_words = {\"gain\", \"gains\", \"bull\", \"bullish\", \"up\", \"upgrade\", \"beat\", \"strong\", \"rally\", \"surge\", \"record\"}\n",
    "negative_words = {\"loss\", \"losses\", \"bear\", \"bearish\", \"down\", \"downgrade\", \"miss\", \"weak\", \"selloff\", \"drop\", \"plunge\"}\n",
    "\n",
    "def sentiment_score(text: str) -> int:\n",
    "    tokens = re.findall(r\"[a-zA-Z']+\", text.lower())\n",
    "    score = sum(1 for t in tokens if t in positive_words) - sum(1 for t in tokens if t in negative_words)\n",
    "    return score\n",
    "\n",
    "df[\"sentiment_score\"] = df[\"text\"].fillna(\"\").apply(sentiment_score)\n",
    "\n",
    "# Define the sentiment summary tool\n",
    "# Hint: Use @tool from langchain.tools. Compute statistics from df[\"sentiment_score\"] (e.g. average).\n",
    "@tool\n",
    "def get_sentiment_summary() -> str:\n",
    "    \"\"\"Summarize local tweet sentiment across the full dataset.\"\"\"\n",
    "    ... # TODO: Fill this in\n",
    "\n",
    "# Write the SYSTEM_PROMPT\n",
    "# Hint: Include goals + rules; Summary must include average sentiment score and key themes.\n",
    "SYSTEM_PROMPT = None # TODO: Fill this in\n",
    "\n",
    "# Initialize the model\n",
    "example_model = init_chat_model(\"gpt-4.1-nano-2025-04-14\", temperature=0, api_key=api_key)\n",
    "\n",
    "# Create the agent and run it\n",
    "# Hint: create_agent(model=example_model, tools=[get_sentiment_summary], system_prompt=SYSTEM_PROMPT)\n",
    "decision_agent = None  # TODO: Fill this in\n",
    "\n",
    "response = decision_agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Use the overall tweet sentiment to decide BUY or NOT BUY SPY.\"}]})\n",
    "print(response[\"messages\"][-1].content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Congratulations \n",
    "You've completed the LangChain tutorial! We covered\n",
    "\n",
    "- How to create agents with LangChain  \n",
    "- How to build custom tools  \n",
    "- How to add memory to agents  \n",
    "- How to use structured output  \n",
    "- How to build a daily decision agent  \n",
    "\n",
    "### Possible next steps to explore\n",
    "   - **LangGraph**: For more complex agent workflows (see the LangGraph notebook!)\n",
    "   - **Retrieval**: Connect agents to vector databases for RAG\n",
    "   - **Multi-agent systems**: Agents that collaborate\n",
    "   - **LangSmith**: Observability and debugging tools\n",
    "\n",
    "### Additional resources\n",
    "   - [LangChain Docs](https://docs.langchain.com)\n",
    "   - [LangChain Quickstart](https://docs.langchain.com/oss/python/langchain/quickstart)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Happy learning!\n",
    "\n",
    "Pedro"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
